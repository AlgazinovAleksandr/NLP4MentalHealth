{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b129b43",
   "metadata": {},
   "source": [
    "### Про метрику\n",
    "\n",
    "Важной частью этого чекпоинта является выбор метрики. Напомним, что на текущий момент видение проекта - LLM / Agent-based ассистент для задач ментального здоровья. То есть в качестве стандартных NLP-метрик необходимо использовать метрики для задачи Language Modeling / текстовой генерации, такие как вывод обученной reward-модели, perplexity, side-by-side сравнение на асессорах, и так далее. При этом для ML-бейзлайна на текущем этапе мы будем решать задачу текстовой классификации. Для этой задачи мы можем использовать классические метрики для текстовой классификации, такие как accuracy, precision, recall, F1-score, и так далее. При этом у нас нет негативых сэмплов, так как каждое наблюдение принадлежит какому-то классу, связанного с ментальным здоровьем. Поэтому будем использовать просто accuracy (сбалансированную по классам, так как датасет является не супер сбалансированным)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9e97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c50bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of reddit_df:  151288\n",
      "len of reddit_df after dropna:  3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125992</th>\n",
       "      <td>afraid dont love girlfriend</td>\n",
       "      <td>deleted</td>\n",
       "      <td>OCD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130950</th>\n",
       "      <td>every single dream kind mental game</td>\n",
       "      <td>“ dream ” last night romanticizing ex boyfrien...</td>\n",
       "      <td>ptsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123732</th>\n",
       "      <td>zoloft worked relatively well id like try some...</td>\n",
       "      <td>deleted</td>\n",
       "      <td>OCD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132932</th>\n",
       "      <td>high anxiety day came nowhere wth</td>\n",
       "      <td>removed</td>\n",
       "      <td>ptsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57854</th>\n",
       "      <td>69 high functioning autistic adolescent want r...</td>\n",
       "      <td>deleted</td>\n",
       "      <td>aspergers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "125992                        afraid dont love girlfriend   \n",
       "130950                every single dream kind mental game   \n",
       "123732  zoloft worked relatively well id like try some...   \n",
       "132932                  high anxiety day came nowhere wth   \n",
       "57854   69 high functioning autistic adolescent want r...   \n",
       "\n",
       "                                                     body     target  \n",
       "125992                                            deleted        OCD  \n",
       "130950  “ dream ” last night romanticizing ex boyfrien...       ptsd  \n",
       "123732                                            deleted        OCD  \n",
       "132932                                            removed       ptsd  \n",
       "57854                                             deleted  aspergers  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нанов в процентах немного, а данных много, поэтому позволим себе просто удалить наны на текущем этапе\n",
    "reddit_df = pd.read_csv('../data/reddit_mental_health_posts_preprocessed.csv')\n",
    "reddit_df = reddit_df[['title', 'body', 'subreddit']]\n",
    "reddit_df = reddit_df.rename(columns={'subreddit': 'target'})\n",
    "print('len of reddit_df: ', len(reddit_df))\n",
    "reddit_df = reddit_df.dropna().sample(10000)\n",
    "print('len of reddit_df after dropna: ', len(reddit_df))\n",
    "reddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bcc142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "OCD           837\n",
       "ADHD          729\n",
       "ptsd          494\n",
       "depression    477\n",
       "aspergers     463\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# видно, что датасет не супер сбалансирован\n",
    "reddit_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392397c",
   "metadata": {},
   "source": [
    "### Соединим title и body, и построим эмбеддинги\n",
    "\n",
    "Для сравнения будем использовать:\n",
    "\n",
    "+ tf-idf\n",
    "\n",
    "+ tf-idf + SVD разложение (понижение размерности)\n",
    "\n",
    "+ Word2Vec с усреднением эмбеддингов по словам\n",
    "\n",
    "+ Простенький трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a4f18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125992</th>\n",
       "      <td>OCD</td>\n",
       "      <td>Title: afraid dont love girlfriend; Body: deleted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130950</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>Title: every single dream kind mental game; Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123732</th>\n",
       "      <td>OCD</td>\n",
       "      <td>Title: zoloft worked relatively well id like t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132932</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>Title: high anxiety day came nowhere wth; Body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57854</th>\n",
       "      <td>aspergers</td>\n",
       "      <td>Title: 69 high functioning autistic adolescent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                               text\n",
       "125992        OCD  Title: afraid dont love girlfriend; Body: deleted\n",
       "130950       ptsd  Title: every single dream kind mental game; Bo...\n",
       "123732        OCD  Title: zoloft worked relatively well id like t...\n",
       "132932       ptsd  Title: high anxiety day came nowhere wth; Body...\n",
       "57854   aspergers  Title: 69 high functioning autistic adolescent..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['text'] = 'Title: ' + reddit_df['title'] + '; Body: ' + reddit_df['body']\n",
    "reddit_df = reddit_df.drop(['title', 'body'], axis=1)\n",
    "reddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ea2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7d5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(df):\n",
    "    results = []\n",
    "    df1 = df.copy()\n",
    "    df2 = df.copy()\n",
    "    df3 = df.copy()\n",
    "    df4 = df.copy()\n",
    "    # Let's start with tf-idf embeddings\n",
    "    print(\"Creating tf-idf embeddings\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df1['text']).toarray()\n",
    "    results.append((df1, \"tf-idf\", X_tfidf))\n",
    "    \n",
    "    # Now let's add SVD to serve as dimensionality reduction\n",
    "    print(\"Creating tf-idf + SVD embeddings\")\n",
    "    svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "    X_tfidf_svd = svd.fit_transform(X_tfidf)\n",
    "    results.append((df2, \"tf-idf+SVD\", X_tfidf_svd))\n",
    "    \n",
    "    # Time to do something more advanced - shall we use transformers?\n",
    "    print(\"Creating Sentence-BERT embeddings\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') # something lightweight so that the laptop does not die\n",
    "    sentences = df3['text'].tolist()\n",
    "    X_sbert = model.encode(sentences, batch_size=32, show_progress_bar=True)\n",
    "    results.append((df3, \"Sentence-BERT\", X_sbert))\n",
    "    \n",
    "    # Word2Vec with averaging is not dead, is it?\n",
    "    print(\"Creating Word2Vec embeddings\")\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in df4['text']]\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=128,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=4,\n",
    "        seed=42\n",
    "    )\n",
    "    # Averaging word vectors\n",
    "    doc_vectors = []\n",
    "    for doc in tokenized_texts:\n",
    "        word_vecs = [w2v_model.wv[word] for word in doc if word in w2v_model.wv]\n",
    "        if len(word_vecs) > 0:\n",
    "            doc_vectors.append(np.mean(word_vecs, axis=0))\n",
    "        else:\n",
    "            doc_vectors.append(np.zeros(w2v_model.vector_size))\n",
    "    X_word2vec = np.array(doc_vectors)\n",
    "    results.append((df4, \"Word2Vec\", X_word2vec))\n",
    "    \n",
    "    print('Success')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bc2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tf-idf embeddings\n",
      "Creating tf-idf + SVD embeddings\n",
      "Creating Sentence-BERT embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5457283990f49919fe1c0e3ff31b6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Word2Vec embeddings\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "embedding_df = create_embeddings(reddit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d352a",
   "metadata": {},
   "source": [
    "### Начнем моделировать. Для каждого из эмбеддингов построим 4 модели:\n",
    "\n",
    "+ Logistic Regression\n",
    "\n",
    "+ KNN\n",
    "\n",
    "+ CatBoost\n",
    "\n",
    "+ MLP (простой MLP из sklearn)\n",
    "\n",
    "После этого измерим качество моделей на тестовой выборке и выберем лучшую. Будем выбирать по balanced accuracy, а выборку будем делить на трейн и тест в соотношении 90:10 (для CatBoost и MLP будем использовать дополнительную валидационную выборку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2e9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d9100",
   "metadata": {},
   "source": [
    "### CatBoost GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45e2eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3) (0.804, 0.809, 0.809)\n",
      "(150, 3) (0.908, 0.914, 0.914)\n",
      "(250, 3) (0.953, 0.957, 0.957)\n",
      "(350, 3) (0.952, 0.957, 0.957)\n",
      "(450, 3) (0.965, 0.969, 0.969)\n",
      "(550, 3) (0.973, 0.975, 0.975)\n",
      "(650, 3) (0.977, 0.981, 0.981)\n",
      "(750, 3) (0.985, 0.988, 0.988)\n",
      "(850, 3) (0.977, 0.981, 0.981)\n",
      "(950, 3) (0.972, 0.975, 0.975)\n",
      "(1050, 3) (0.98, 0.981, 0.981)\n",
      "(50, 4) (0.868, 0.87, 0.87)\n",
      "(150, 4) (0.965, 0.969, 0.969)\n",
      "(250, 4) (0.973, 0.975, 0.975)\n",
      "(350, 4) (0.992, 0.994, 0.994)\n",
      "(450, 4) (0.992, 0.994, 0.994)\n",
      "(550, 4) (0.977, 0.981, 0.981)\n",
      "(650, 4) (0.984, 0.988, 0.988)\n",
      "(750, 4) (0.984, 0.988, 0.988)\n",
      "(850, 4) (0.98, 0.981, 0.981)\n",
      "(950, 4) (0.984, 0.988, 0.988)\n",
      "(1050, 4) (0.984, 0.988, 0.988)\n",
      "(50, 5) (0.913, 0.92, 0.92)\n",
      "(150, 5) (0.988, 0.988, 0.988)\n",
      "(250, 5) (0.984, 0.988, 0.988)\n",
      "(350, 5) (0.972, 0.975, 0.975)\n",
      "(450, 5) (0.984, 0.988, 0.988)\n",
      "(550, 5) (0.984, 0.988, 0.988)\n",
      "(650, 5) (0.98, 0.981, 0.981)\n",
      "(750, 5) (0.984, 0.988, 0.988)\n",
      "(850, 5) (0.984, 0.988, 0.988)\n",
      "(950, 5) (0.984, 0.988, 0.988)\n",
      "(1050, 5) (0.984, 0.988, 0.988)\n",
      "(50, 6) (0.965, 0.969, 0.969)\n",
      "(150, 6) (0.992, 0.994, 0.994)\n",
      "(250, 6) (0.984, 0.988, 0.988)\n",
      "(350, 6) (0.984, 0.988, 0.988)\n",
      "(450, 6) (0.98, 0.981, 0.981)\n",
      "(550, 6) (0.984, 0.988, 0.988)\n",
      "(650, 6) (0.98, 0.981, 0.981)\n",
      "(750, 6) (0.984, 0.988, 0.988)\n",
      "(850, 6) (0.984, 0.988, 0.988)\n",
      "(950, 6) (0.98, 0.981, 0.981)\n",
      "(1050, 6) (0.98, 0.981, 0.981)\n",
      "(50, 7) (0.98, 0.981, 0.981)\n",
      "(150, 7) (0.98, 0.981, 0.981)\n",
      "(250, 7) (0.98, 0.981, 0.981)\n",
      "(350, 7) (0.984, 0.988, 0.988)\n",
      "(450, 7) (0.98, 0.981, 0.981)\n",
      "(550, 7) (0.98, 0.981, 0.981)\n",
      "(650, 7) (0.984, 0.988, 0.988)\n",
      "(750, 7) (0.98, 0.981, 0.981)\n",
      "(850, 7) (0.98, 0.981, 0.981)\n",
      "(950, 7) (0.984, 0.988, 0.988)\n",
      "(1050, 7) (0.984, 0.988, 0.988)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for max_depth in range(3, 8):\n",
    "    for iterations in range(50, 1051, 100):\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=20\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        accuracy = round(balanced_accuracy_score(y_val, y_pred), 3)\n",
    "        precision = round(precision_score(y_val, y_pred, average='micro'), 3)\n",
    "        recall = round(recall_score(y_val, y_pred, average='micro'), 3)\n",
    "        \n",
    "        results.append(\n",
    "            ((iterations, max_depth), (accuracy, precision, recall))\n",
    "        )\n",
    "        \n",
    "        print((iterations, max_depth), (accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55531e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((350, 4), (0.992, 0.994, 0.994)),\n",
       " ((450, 4), (0.992, 0.994, 0.994)),\n",
       " ((150, 6), (0.992, 0.994, 0.994))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results, key=lambda val: val[1][2])[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef5608",
   "metadata": {},
   "source": [
    "#### Итого, лучшие гиперпараметры - 150 деревьев глубиной до 6\n",
    "\n",
    "P.S. Отбор происходил по recall, потому что в нашей задаче будто бы цена FN ошибки намного больше, чем FP. Лучше человека случайно причислить к больным и отправить на доп. обследования, чем объявить здоровым несмотря на болезнь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f08b1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, embedding_name, model_name, \n",
    "                             X_val=None, y_val=None):\n",
    "    \n",
    "    if model_name == \"Logistic Regression\":\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"CatBoost\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=500,\n",
    "                max_depth=7,\n",
    "                random_state=42,\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=20\n",
    "            )\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        else:\n",
    "            model = CatBoostClassifier(random_state=42, verbose=False)\n",
    "            model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=16) # 2 ** 4\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"MLP\":\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64),\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.05,\n",
    "            max_iter=100\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    return accuracy, precision, recall, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25110ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tf-idf+SVD\n",
      "Train shape: (2538, 300), Val shape: (162, 300), Test shape: (300, 300)\n",
      "==============================\n",
      "Training Logistic Regression\n",
      "tf-idf+SVD + Logistic Regression\n",
      "Acc: 0.6180, Pr:0.6467, Rec:0.6467\n",
      "[[53  4  7  6  3]\n",
      " [ 7 65  5  4  3]\n",
      " [13  6 21  4  2]\n",
      " [ 6  7  2 26  7]\n",
      " [ 7  5  4  4 29]]\n",
      "==============================\n",
      "Training CatBoost\n",
      "tf-idf+SVD + CatBoost\n",
      "Acc: 0.6327, Pr:0.6533, Rec:0.6533\n",
      "[[50  6  7  6  4]\n",
      " [ 3 64  5  7  5]\n",
      " [13  1 23  7  2]\n",
      " [10  3  2 30  3]\n",
      " [ 6  4  3  7 29]]\n",
      "==============================\n",
      "Training KNN\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([X_train, X_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     29\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y_train, y_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# объединяем трейн и валидацию для всех моделей кроме СB\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     accuracy, precision, recall, conf_matrix \u001b[38;5;241m=\u001b[39m train_and_evaluate_models(\n\u001b[1;32m     31\u001b[0m         X_train, X_test, y_train, y_test, embedding_name, model_name\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding_name,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion_Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m: conf_matrix\n\u001b[1;32m     40\u001b[0m })\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m, in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(X_train, X_test, y_train, y_test, embedding_name, model_name, X_val, y_val)\u001b[0m\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m) \u001b[38;5;66;03m# 2 ** 4\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 27\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     model \u001b[38;5;241m=\u001b[39m MLPClassifier(\n\u001b[1;32m     31\u001b[0m         hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m     32\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:249\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ArgKminClassMode\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    247\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m    248\u001b[0m     ):\n\u001b[0;32m--> 249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    252\u001b[0m                 [\n\u001b[1;32m    253\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[idx][np\u001b[38;5;241m.\u001b[39margmax(probas, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    257\u001b[0m             )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:350\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    351\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    818\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 822\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    823\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    824\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m    825\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[1;32m    826\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[1;32m    827\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[1;32m    828\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    829\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    830\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    834\u001b[0m ):\n\u001b[1;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    836\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    837\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:258\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    259\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    260\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m    261\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    262\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    263\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    264\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[1;32m    265\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m    266\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    271\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    272\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    279\u001b[0m     )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:90\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:72\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[0;34m(limits, user_api)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m threadpoolctl\u001b[38;5;241m.\u001b[39mthreadpool_limits(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[0;34m(self, limits, user_api)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_threadpool_limits()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m _ThreadpoolInfo(prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes,\n\u001b[1;32m    269\u001b[0m                           user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_modules()\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_module_from_path(filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(filepath, prefix, user_api, internal_api)\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_version()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "models = [\"Logistic Regression\", \"CatBoost\", \"KNN\", \"MLP\"]\n",
    "results = []\n",
    "\n",
    "for df_copy, embedding_name, X_embeddings in embedding_df:\n",
    "    if embedding_name == \"tf-idf\": # неудачная размерность 5000, все слишком долго обучалось и я решил выкинуть обычный tf-idf из анализа\n",
    "        continue\n",
    "    print(f\"Processing {embedding_name}\")\n",
    "    y = df_copy['target'].values\n",
    "    # Везде будет одинаковая тестовая выборка даже при условии, что валидационную использует по сути только CatBoost\n",
    "    # А нейронка делает валидационную саму из трейновой выборке. Это не супер корректно, но мб кто-то это когда-то поправит :)\n",
    "    # Но в целом и так не критично, у нас же бейзлайн\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_embeddings, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.06, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Training {model_name}\")\n",
    "        if model_name in [\"CatBoost\"]:\n",
    "            accuracy, precision, recall, conf_matrix = train_and_evaluate_models(\n",
    "                X_train, X_test, y_train, y_test, embedding_name, model_name, X_val, y_val\n",
    "            )\n",
    "        else:\n",
    "            X_train = np.concatenate([X_train, X_val], axis=0)\n",
    "            y_train = np.concatenate([y_train, y_val], axis=0) # объединяем трейн и валидацию для всех моделей кроме СB\n",
    "            accuracy, precision, recall, conf_matrix = train_and_evaluate_models(\n",
    "                X_train, X_test, y_train, y_test, embedding_name, model_name\n",
    "            )\n",
    "        results.append({\n",
    "            'Embedding': embedding_name,\n",
    "            'Model': model_name,\n",
    "            'Balanced_Accuracy': accuracy,\n",
    "            'Micro_Precision': precision,\n",
    "            'Micro_Recall': recall,\n",
    "            'Confusion_Matrix': conf_matrix\n",
    "        })\n",
    "        print(f\"{embedding_name} + {model_name}\")\n",
    "        print(f\"Acc: {accuracy:.4f}, Pr:{precision:.4f}, Rec:{recall:.4f}\")\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2df56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "pivot_results = results_df.pivot_table(\n",
    "        index='Embedding', \n",
    "        columns='Model', \n",
    "        values='Balanced_Accuracy'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dddb7e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence-BERT</th>\n",
       "      <td>0.749341</td>\n",
       "      <td>0.728169</td>\n",
       "      <td>0.747167</td>\n",
       "      <td>0.762219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.690497</td>\n",
       "      <td>0.597548</td>\n",
       "      <td>0.684865</td>\n",
       "      <td>0.713112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf-idf+SVD</th>\n",
       "      <td>0.743215</td>\n",
       "      <td>0.583596</td>\n",
       "      <td>0.745709</td>\n",
       "      <td>0.756078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model          CatBoost       KNN  Logistic Regression       MLP\n",
       "Embedding                                                       \n",
       "Sentence-BERT  0.749341  0.728169             0.747167  0.762219\n",
       "Word2Vec       0.690497  0.597548             0.684865  0.713112\n",
       "tf-idf+SVD     0.743215  0.583596             0.745709  0.756078"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff12ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_results.to_csv('reddit_baseline_results_balanced_accuracy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4160c",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "+ MLP (на удивление) оказалась и самой сильной, и самой устойчивой моделью. В отличие от всех других моделей, здесь качество (в виде balanced accuracy) на тестовой выборке не опустилось ниже 0.7\n",
    "\n",
    "+ KNN проигрывает всем моделям на всех эмбеддингах, единственный показал качество ниже 0.6. Это значительно хуже других моделей\n",
    "\n",
    "+ Он же (KNN) оказался самым неустойчивым, где разброс между лучшей и худшей моделью превышает 0.1 (во всех остальных моделях спред в районе 0.05)\n",
    "\n",
    "+ Несмотря на кардинально разные алгоритмы и структуры моделей, CatBoost и LogReg ведут себя удивительно схоже с точки зрения качества\n",
    "\n",
    "+ Хоть мы и обучили немного всего, это честный труд :) А еще мы получили на удивление действительно интересные результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cbe83",
   "metadata": {},
   "source": [
    "### Возможный (и желательный) TODO:\n",
    "\n",
    "+ DONE - Пофиксить логику с валидационной выборкой. Пусть она создается не везде, а только там, где нужно, чтобы не терять данные\n",
    "\n",
    "+ DONE - Измерить больше метрик, построить confusion матрицы\n",
    "\n",
    "+ Выбрать лучшую модель (например, CatBoost / MLP на Sentence-BERT эмбеддингах), потюнить ее гиперпараметры, и посмотреть, как будет меняться качество. Выбрать лучшие гиперпараметры, и замерить финальное качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
