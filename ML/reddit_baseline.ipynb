{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b129b43",
   "metadata": {},
   "source": [
    "### Про метрику\n",
    "\n",
    "Важной частью этого чекпоинта является выбор метрики. Напомним, что на текущий момент видение проекта - LLM / Agent-based ассистент для задач ментального здоровья. То есть в качестве стандартных NLP-метрик необходимо использовать метрики для задачи Language Modeling / текстовой генерации, такие как вывод обученной reward-модели, perplexity, side-by-side сравнение на асессорах, и так далее. При этом для ML-бейзлайна на текущем этапе мы будем решать задачу текстовой классификации. Для этой задачи мы можем использовать классические метрики для текстовой классификации, такие как accuracy, precision, recall, F1-score, и так далее. При этом у нас нет негативых сэмплов, так как каждое наблюдение принадлежит какому-то классу, связанного с ментальным здоровьем. Поэтому будем использовать просто accuracy (сбалансированную по классам, так как датасет является не супер сбалансированным)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9e97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c50bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of reddit_df:  151288\n",
      "len of reddit_df after dropna:  148936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get extremely anxious ’ working 247</td>\n",
       "      <td>month ago accepted full time software engineer...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cant clean house feel incredibly motivated cle...</td>\n",
       "      <td>hey guy curious anyone else issue apartment fu...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>need help</td>\n",
       "      <td>6 exam next 2 week one monday havent studied f...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anyone chat</td>\n",
       "      <td>anyone struggling addadhd ’ interesting chatti...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>figuring eat suck</td>\n",
       "      <td>whenever get hungry never eat dont know eat en...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                get extremely anxious ’ working 247   \n",
       "1  cant clean house feel incredibly motivated cle...   \n",
       "2                                          need help   \n",
       "3                                        anyone chat   \n",
       "4                                  figuring eat suck   \n",
       "\n",
       "                                                body target  \n",
       "0  month ago accepted full time software engineer...   ADHD  \n",
       "1  hey guy curious anyone else issue apartment fu...   ADHD  \n",
       "2  6 exam next 2 week one monday havent studied f...   ADHD  \n",
       "3  anyone struggling addadhd ’ interesting chatti...   ADHD  \n",
       "4  whenever get hungry never eat dont know eat en...   ADHD  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нанов в процентах немного, а данных много, поэтому позволим себе просто удалить наны на текущем этапе\n",
    "reddit_df = pd.read_csv('../data/reddit_mental_health_posts_preprocessed.csv')\n",
    "reddit_df = reddit_df[['title', 'body', 'subreddit']]\n",
    "reddit_df = reddit_df.rename(columns={'subreddit': 'target'})\n",
    "print('len of reddit_df: ', len(reddit_df))\n",
    "reddit_df = reddit_df.dropna()#.sample(1000)\n",
    "print('len of reddit_df after dropna: ', len(reddit_df))\n",
    "reddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bcc142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "OCD           41812\n",
       "ADHD          37058\n",
       "depression    23770\n",
       "ptsd          23758\n",
       "aspergers     22538\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# видно, что датасет не супер сбалансирован\n",
    "reddit_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392397c",
   "metadata": {},
   "source": [
    "### Соединим title и body, и построим эмбеддинги\n",
    "\n",
    "Для сравнения будем использовать:\n",
    "\n",
    "+ tf-idf\n",
    "\n",
    "+ tf-idf + SVD разложение (понижение размерности)\n",
    "\n",
    "+ Word2Vec с усреднением эмбеддингов по словам\n",
    "\n",
    "+ Простенький трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a4f18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>Title: get extremely anxious ’ working 247; Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>Title: cant clean house feel incredibly motiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>Title: need help; Body: 6 exam next 2 week one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>Title: anyone chat; Body: anyone struggling ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>Title: figuring eat suck; Body: whenever get h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0   ADHD  Title: get extremely anxious ’ working 247; Bo...\n",
       "1   ADHD  Title: cant clean house feel incredibly motiva...\n",
       "2   ADHD  Title: need help; Body: 6 exam next 2 week one...\n",
       "3   ADHD  Title: anyone chat; Body: anyone struggling ad...\n",
       "4   ADHD  Title: figuring eat suck; Body: whenever get h..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['text'] = 'Title: ' + reddit_df['title'] + '; Body: ' + reddit_df['body']\n",
    "reddit_df = reddit_df.drop(['title', 'body'], axis=1)\n",
    "reddit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ea2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7d5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(df):\n",
    "    results = []\n",
    "    df1 = df.copy()\n",
    "    df2 = df.copy()\n",
    "    df3 = df.copy()\n",
    "    df4 = df.copy()\n",
    "    # Let's start with tf-idf embeddings\n",
    "    print(\"Creating tf-idf embeddings\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df1['text']).toarray()\n",
    "    results.append((df1, \"tf-idf\", X_tfidf))\n",
    "    \n",
    "    # Now let's add SVD to serve as dimensionality reduction\n",
    "    print(\"Creating tf-idf + SVD embeddings\")\n",
    "    svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "    X_tfidf_svd = svd.fit_transform(X_tfidf)\n",
    "    results.append((df2, \"tf-idf+SVD\", X_tfidf_svd))\n",
    "    \n",
    "    # Time to do something more advanced - shall we use transformers?\n",
    "    print(\"Creating Sentence-BERT embeddings\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') # something lightweight so that the laptop does not die\n",
    "    sentences = df3['text'].tolist()\n",
    "    X_sbert = model.encode(sentences, batch_size=32, show_progress_bar=True)\n",
    "    results.append((df3, \"Sentence-BERT\", X_sbert))\n",
    "    \n",
    "    # Word2Vec with averaging is not dead, is it?\n",
    "    print(\"Creating Word2Vec embeddings\")\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in df4['text']]\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=128,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=4,\n",
    "        seed=42\n",
    "    )\n",
    "    # Averaging word vectors\n",
    "    doc_vectors = []\n",
    "    for doc in tokenized_texts:\n",
    "        word_vecs = [w2v_model.wv[word] for word in doc if word in w2v_model.wv]\n",
    "        if len(word_vecs) > 0:\n",
    "            doc_vectors.append(np.mean(word_vecs, axis=0))\n",
    "        else:\n",
    "            doc_vectors.append(np.zeros(w2v_model.vector_size))\n",
    "    X_word2vec = np.array(doc_vectors)\n",
    "    results.append((df4, \"Word2Vec\", X_word2vec))\n",
    "    \n",
    "    print('Success')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bc2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tf-idf embeddings\n",
      "Creating tf-idf + SVD embeddings\n",
      "Creating Sentence-BERT embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fc7513cf5a49168a8e7860d082d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Word2Vec embeddings\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "embedding_df = create_embeddings(reddit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d352a",
   "metadata": {},
   "source": [
    "### Начнем моделировать. Для каждого из эмбеддингов построим 4 модели:\n",
    "\n",
    "+ Logistic Regression\n",
    "\n",
    "+ KNN\n",
    "\n",
    "+ CatBoost\n",
    "\n",
    "+ MLP (простой MLP из sklearn)\n",
    "\n",
    "После этого измерим качество моделей на тестовой выборке и выберем лучшую. Будем выбирать по balanced accuracy, а выборку будем делить на трейн и тест в соотношении 90:10 (для CatBoost и MLP будем использовать дополнительную валидационную выборку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2e9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d9100",
   "metadata": {},
   "source": [
    "### CatBoost GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e2eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for max_depth in range(4, 8):\n",
    "    for iterations in range(500, 1051, 100):\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=20\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        accuracy = round(balanced_accuracy_score(y_val, y_pred), 3)\n",
    "        precision = round(precision_score(y_val, y_pred, average='micro'), 3)\n",
    "        recall = round(recall_score(y_val, y_pred, average='micro'), 3)\n",
    "        \n",
    "        results.append(\n",
    "            ((iterations, max_depth), (accuracy, precision, recall))\n",
    "        )\n",
    "        \n",
    "        print((iterations, max_depth), (accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55531e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(results, key=lambda val: val[1][2])[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef5608",
   "metadata": {},
   "source": [
    "#### Итого, лучшие гиперпараметры - 900 деревьев глубиной до 7\n",
    "\n",
    "P.S. Отбор происходил по recall, потому что в нашей задаче будто бы цена FN ошибки намного больше, чем FP. Лучше человека случайно причислить к больным и отправить на доп. обследования, чем объявить здоровым несмотря на болезнь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe968cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08b1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test, embedding_name, model_name, \n",
    "                             X_val=None, y_val=None):\n",
    "    \n",
    "    if model_name == \"Logistic Regression\":\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"CatBoost\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=900,\n",
    "                max_depth=7,\n",
    "                random_state=42,\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=20\n",
    "            )\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "        else:\n",
    "            model = CatBoostClassifier(random_state=42, verbose=False)\n",
    "            model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=16) # 2 ** 4\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    elif model_name == \"MLP\":\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64),\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.05,\n",
    "            max_iter=100\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    return accuracy, precision, recall, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25110ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tf-idf+SVD\n",
      "Train shape: (125999, 300), Val shape: (8043, 300), Test shape: (14894, 300)\n",
      "==============================\n",
      "Training KNN\n",
      "tf-idf+SVD + KNN\n",
      "Acc: 0.5834, Pr:0.6242, Rec:0.6166\n",
      "[[2619  339  490  176   82]\n",
      " [ 254 3201  332  249  145]\n",
      " [ 378  326 1279  196   75]\n",
      " [ 386  404  437 1018  132]\n",
      " [ 287  433  356  233 1067]]\n",
      "==============================\n",
      "Training Logistic Regression\n",
      "tf-idf+SVD + Logistic Regression\n",
      "Acc: 0.7473, Pr:0.7692, Rec:0.7684\n",
      "[[3097  158  218  172   61]\n",
      " [ 148 3556  132  229  116]\n",
      " [ 310  206 1472  205   61]\n",
      " [ 218  244  140 1668  107]\n",
      " [ 140  254  100  230 1652]]\n",
      "==============================\n",
      "Training CatBoost\n",
      "tf-idf+SVD + CatBoost\n",
      "Acc: 0.7454, Pr:0.7677, Rec:0.7667\n",
      "[[3069  156  227  188   66]\n",
      " [ 119 3570  142  233  117]\n",
      " [ 284  185 1468  235   82]\n",
      " [ 196  211  148 1695  127]\n",
      " [ 143  245  124  247 1617]]\n",
      "==============================\n",
      "Training MLP\n",
      "tf-idf+SVD + MLP\n",
      "Acc: 0.7566, Pr:0.7750, Rec:0.7761\n",
      "[[3093  123  233  164   93]\n",
      " [ 114 3570  118  194  185]\n",
      " [ 267  152 1506  195  134]\n",
      " [ 182  217  154 1603  221]\n",
      " [ 102  194   92  201 1787]]\n",
      "Processing Sentence-BERT\n",
      "Train shape: (125999, 384), Val shape: (8043, 384), Test shape: (14894, 384)\n",
      "==============================\n",
      "Training KNN\n",
      "Sentence-BERT + KNN\n",
      "Acc: 0.7309, Pr:0.7629, Rec:0.7563\n",
      "[[3060  203  124  235   84]\n",
      " [ 148 3580   51  300  102]\n",
      " [ 379  281 1175  316  103]\n",
      " [ 169  204   83 1787  134]\n",
      " [ 113  272   51  278 1662]]\n",
      "==============================\n",
      "Training Logistic Regression\n",
      "Sentence-BERT + Logistic Regression\n",
      "Acc: 0.7461, Pr:0.7660, Rec:0.7654\n",
      "[[3059  159  220  205   63]\n",
      " [ 138 3509  147  226  161]\n",
      " [ 307  199 1427  239   82]\n",
      " [ 175  213  140 1713  136]\n",
      " [ 110  232  113  229 1692]]\n",
      "==============================\n",
      "Training CatBoost\n",
      "Sentence-BERT + CatBoost\n",
      "Acc: 0.7505, Pr:0.7701, Rec:0.7698\n",
      "[[3094  150  210  184   68]\n",
      " [ 145 3516  136  234  150]\n",
      " [ 304  188 1452  222   88]\n",
      " [ 173  207  155 1703  139]\n",
      " [ 104  222  119  230 1701]]\n",
      "==============================\n",
      "Training MLP\n",
      "Sentence-BERT + MLP\n",
      "Acc: 0.7677, Pr:0.7882, Rec:0.7840\n",
      "[[3134   87  252  186   47]\n",
      " [ 140 3513  169  247  112]\n",
      " [ 274  134 1573  214   59]\n",
      " [ 171  150  194 1753  109]\n",
      " [ 116  180  148  228 1704]]\n",
      "Processing Word2Vec\n",
      "Train shape: (125999, 128), Val shape: (8043, 128), Test shape: (14894, 128)\n",
      "==============================\n",
      "Training KNN\n",
      "Word2Vec + KNN\n",
      "Acc: 0.6001, Pr:0.6312, Rec:0.6348\n",
      "[[2847  305  278  163  113]\n",
      " [ 429 3140  147  270  195]\n",
      " [ 548  359  981  207  159]\n",
      " [ 354  372  132 1335  184]\n",
      " [ 298  502  151  274 1151]]\n",
      "==============================\n",
      "Training Logistic Regression\n",
      "Word2Vec + Logistic Regression\n",
      "Acc: 0.6887, Pr:0.7092, Rec:0.7104\n",
      "[[2885  190  305  207  119]\n",
      " [ 253 3316  148  252  212]\n",
      " [ 452  171 1261  214  156]\n",
      " [ 224  232  158 1592  171]\n",
      " [ 166  299  137  247 1527]]\n",
      "==============================\n",
      "Training CatBoost\n",
      "Word2Vec + CatBoost\n",
      "Acc: 0.6986, Pr:0.7205, Rec:0.7214\n",
      "[[2928  188  278  205  107]\n",
      " [ 212 3386  133  273  177]\n",
      " [ 399  196 1274  238  147]\n",
      " [ 207  248  145 1628  149]\n",
      " [ 165  287  141  255 1528]]\n",
      "==============================\n",
      "Training MLP\n",
      "Word2Vec + MLP\n",
      "Acc: 0.7162, Pr:0.7382, Rec:0.7384\n",
      "[[3033  134  289  177   73]\n",
      " [ 218 3411  132  265  155]\n",
      " [ 399  151 1354  205  145]\n",
      " [ 250  205  149 1618  155]\n",
      " [ 181  255  139  220 1581]]\n"
     ]
    }
   ],
   "source": [
    "models = [\"KNN\", \"Logistic Regression\", \"CatBoost\", \"MLP\"]\n",
    "results = []\n",
    "\n",
    "for df_copy, embedding_name, X_embeddings in embedding_df:\n",
    "    if embedding_name == \"tf-idf\": # неудачная размерность 5000, все слишком долго обучалось и я решил выкинуть обычный tf-idf из анализа\n",
    "        continue\n",
    "    print(f\"Processing {embedding_name}\")\n",
    "    y = df_copy['target'].values\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_embeddings, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.06, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "    val_merged = False\n",
    "    for model_name in models:\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Training {model_name}\")\n",
    "        if model_name in [\"CatBoost\"]:\n",
    "            accuracy, precision, recall, conf_matrix = train_and_evaluate_models(\n",
    "                X_train, X_test, y_train, y_test, embedding_name, model_name, X_val, y_val\n",
    "            )\n",
    "        else:\n",
    "            if not val_merged:\n",
    "                X_train = np.concatenate([X_train, X_val], axis=0)\n",
    "                y_train = np.concatenate([y_train, y_val], axis=0) # объединяем трейн и валидацию для всех моделей кроме СB\n",
    "                val_merged = True\n",
    "\n",
    "            accuracy, precision, recall, conf_matrix = train_and_evaluate_models(\n",
    "                X_train, X_test, y_train, y_test, embedding_name, model_name\n",
    "            )\n",
    "        results.append({\n",
    "            'Embedding': embedding_name,\n",
    "            'Model': model_name,\n",
    "            'Balanced_Accuracy': accuracy,\n",
    "            'Weighted_Precision': precision,\n",
    "            'Weighted_Recall': recall,\n",
    "            'Confusion_Matrix': conf_matrix\n",
    "        })\n",
    "        print(f\"{embedding_name} + {model_name}\")\n",
    "        print(f\"Acc: {accuracy:.4f}, Pr:{precision:.4f}, Rec:{recall:.4f}\")\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2df56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51557885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence-BERT</th>\n",
       "      <td>0.769840</td>\n",
       "      <td>0.756278</td>\n",
       "      <td>0.765409</td>\n",
       "      <td>0.784007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.721364</td>\n",
       "      <td>0.634752</td>\n",
       "      <td>0.710420</td>\n",
       "      <td>0.738351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf-idf+SVD</th>\n",
       "      <td>0.766685</td>\n",
       "      <td>0.616624</td>\n",
       "      <td>0.768430</td>\n",
       "      <td>0.776084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model          CatBoost       KNN  Logistic Regression       MLP\n",
       "Embedding                                                       \n",
       "Sentence-BERT  0.769840  0.756278             0.765409  0.784007\n",
       "Word2Vec       0.721364  0.634752             0.710420  0.738351\n",
       "tf-idf+SVD     0.766685  0.616624             0.768430  0.776084"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.pivot_table(\n",
    "        index='Embedding', \n",
    "        columns='Model', \n",
    "        values='Weighted_Recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dddb7e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence-BERT</th>\n",
       "      <td>0.750471</td>\n",
       "      <td>0.730904</td>\n",
       "      <td>0.746113</td>\n",
       "      <td>0.767682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.698627</td>\n",
       "      <td>0.600103</td>\n",
       "      <td>0.688691</td>\n",
       "      <td>0.716208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf-idf+SVD</th>\n",
       "      <td>0.745381</td>\n",
       "      <td>0.583416</td>\n",
       "      <td>0.747252</td>\n",
       "      <td>0.756617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model          CatBoost       KNN  Logistic Regression       MLP\n",
       "Embedding                                                       \n",
       "Sentence-BERT  0.750471  0.730904             0.746113  0.767682\n",
       "Word2Vec       0.698627  0.600103             0.688691  0.716208\n",
       "tf-idf+SVD     0.745381  0.583416             0.747252  0.756617"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.pivot_table(\n",
    "        index='Embedding', \n",
    "        columns='Model', \n",
    "        values='Balanced_Accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a52383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence-BERT</th>\n",
       "      <td>0.770112</td>\n",
       "      <td>0.762946</td>\n",
       "      <td>0.765976</td>\n",
       "      <td>0.788205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>0.720500</td>\n",
       "      <td>0.631221</td>\n",
       "      <td>0.709199</td>\n",
       "      <td>0.738150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf-idf+SVD</th>\n",
       "      <td>0.767727</td>\n",
       "      <td>0.624210</td>\n",
       "      <td>0.769195</td>\n",
       "      <td>0.774979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model          CatBoost       KNN  Logistic Regression       MLP\n",
       "Embedding                                                       \n",
       "Sentence-BERT  0.770112  0.762946             0.765976  0.788205\n",
       "Word2Vec       0.720500  0.631221             0.709199  0.738150\n",
       "tf-idf+SVD     0.767727  0.624210             0.769195  0.774979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.pivot_table(\n",
    "        index='Embedding', \n",
    "        columns='Model', \n",
    "        values='Weighted_Precision'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da22da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff12ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('reddit_baseline_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4160c",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "\n",
    "+ MLP (на удивление) оказалась и самой сильной, и самой устойчивой моделью. В отличие от всех других моделей, здесь качество (в виде balanced accuracy) на тестовой выборке не опустилось ниже 0.7\n",
    "\n",
    "+ KNN проигрывает всем моделям на всех эмбеддингах, единственный показал качество ниже 0.6. Это значительно хуже других моделей\n",
    "\n",
    "+ Он же (KNN) оказался самым неустойчивым, где разброс между лучшей и худшей моделью превышает 0.1 (во всех остальных моделях спред в районе 0.05)\n",
    "\n",
    "+ Несмотря на кардинально разные алгоритмы и структуры моделей, CatBoost и LogReg ведут себя удивительно схоже с точки зрения качества\n",
    "\n",
    "+ Хоть мы и обучили немного всего, это честный труд :) А еще мы получили на удивление действительно интересные результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cbe83",
   "metadata": {},
   "source": [
    "### Возможный (и желательный) TODO:\n",
    "\n",
    "+ DONE - Пофиксить логику с валидационной выборкой. Пусть она создается не везде, а только там, где нужно, чтобы не терять данные\n",
    "\n",
    "+ DONE - Измерить больше метрик, построить confusion матрицы\n",
    "\n",
    "+ Выбрать лучшую модель (например, CatBoost / MLP на Sentence-BERT эмбеддингах), потюнить ее гиперпараметры, и посмотреть, как будет меняться качество. Выбрать лучшие гиперпараметры, и замерить финальное качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
